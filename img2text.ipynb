{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from CLIP.clip import load, tokenize\n",
    "from CLIP.clip.simple_tokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, transform = load(\"ViT-B/32\", jit=False, device=device)\n",
    "model = model.eval().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(model, embedding):\n",
    "    # from embed weights to tokens\n",
    "    embed_weights = model.token_embedding.weight.data\n",
    "    if embedding.ndim > 2:\n",
    "        embedding = embedding.squeeze(0)\n",
    "    decoded_tokens = np.array([torch.argmin(torch.norm(embed_weights - e, dim=-1)).item() for e in embedding])\n",
    "    decoded_tokens = decoded_tokens[decoded_tokens != 0][1:-1] # ignore empty stops, start, and end token\n",
    "    decoded_text = tokenizer.decode(decoded_tokens)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "base_images = os.listdir(\"base_images\")\n",
    "print(base_images)\n",
    "img_name = base_images[-1]\n",
    "img = Image.open(\"base_images/\" + img_name).convert(\"RGB\")\n",
    "norm = transform.transforms[-1]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create start text\n",
    "start_token = 1\n",
    "text = \"A picture of a cat.\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "embed = model.embed_text(tokens.to(device)).detach()\n",
    "embed_std = embed.std()\n",
    "embed_mean = embed.mean()\n",
    "\n",
    "max_idx = tokens.argmax()\n",
    "opt = embed[0, start_token: max_idx].clone().detach().requires_grad_(True)\n",
    "opt_norm = opt.detach().norm(dim=-1).mean().item()\n",
    "opt_mean = opt.detach().mean(dim=-1).mean().item()\n",
    "opt_std = opt.detach().std(dim=-1).mean().item()\n",
    "\n",
    "embed[0, start_token: max_idx] = 0\n",
    "print(opt.shape)\n",
    "print(opt_norm)\n",
    "print(opt.mean())\n",
    "print(opt.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.Compose([#T.Resize((224, 224)),\n",
    "               T.RandomResizedCrop(224, scale=(0.6, 1.0), ratio=(0.9, 1.1)),\n",
    "               \n",
    "    \n",
    "               #T.RandomAffine([0, 10], \n",
    "               #     translate=(0, 0.3),\n",
    "               #     scale=(0.8, 1.0), \n",
    "               #     shear=(0.5, 0.8),\n",
    "               #     fillcolor=255),\n",
    "              #T.RandomGrayscale(p=0.2),\n",
    "              #T.RandomPerspective(distortion_scale=0.3,\n",
    "              #                    p=0.3,\n",
    "              #                    fill=255),\n",
    "               T.ToTensor(),\n",
    "               norm,\n",
    "            ])\n",
    "\n",
    "norm_img = norm(T.ToTensor()(img.resize((224, 224)))).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    norm_img_feats = model.encode_image(norm_img)\n",
    "\n",
    "# demonstrate transform\n",
    "#T.ToPILImage()(t(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = embed[0, :start_token]\n",
    "suffix = embed[0, max_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def augment_text(opt, prefix, suffix, n_gram=1):\n",
    "    pos = random.randint(0, len(opt) - n_gram)\n",
    "\n",
    "    opt_part = opt[pos: pos + n_gram]\n",
    "\n",
    "    new_emb = torch.cat([prefix, opt_part, suffix,])\n",
    "    size = n_gram + len(prefix) + len(suffix)\n",
    "    if size < 77:\n",
    "        new_emb = torch.cat([new_emb, torch.stack([suffix[-1].clone() for _ in range(77 - size)])])\n",
    "        \n",
    "    return new_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "steps = 100\n",
    "bs = 16\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([opt], lr=lr)#, weight_decay=0.2)\n",
    "\n",
    "\n",
    "best_text = None\n",
    "best_text_loss = 100\n",
    "aug_losses = []\n",
    "losses = []\n",
    "text_losses = []\n",
    "reg_losses = []\n",
    "\n",
    "embed_weights = model.token_embedding.weight.data\n",
    "\n",
    "\n",
    "pbar = trange(steps)\n",
    "for step in range(steps):\n",
    "    embedding = embed.clone()\n",
    "    embedding[0, start_token:max_idx] += opt\n",
    "    \n",
    "    embedding_batch = augment_text(opt, prefix, suffix, n_gram=len(opt)).unsqueeze(0)\n",
    "    #embedding_batch = torch.stack([augment_text(opt, prefix, suffix, n_gram=random.randint(1, len(opt))) for _ in range(bs)])\n",
    "    \n",
    "    #embedding_batch = torch.cat([embedding + torch.zeros_like(embedding).normal_(mean=0, std=embed_std.item() / 10) for _ in range(bs)])\n",
    "    text_feats = model.encode_text(tokens, embedding=embedding_batch).to(device)\n",
    "    \n",
    "    img_batch = img\n",
    "    img_batch = torch.stack([t(img) for _ in range(bs)])\n",
    "    with torch.no_grad():\n",
    "        img_feats = model.encode_image(img_batch.to(device))\n",
    "        \n",
    "    norm_loss = (opt.norm(dim=-1).mean() - opt_norm) ** 2\n",
    "    mean_loss = (opt.mean(dim=-1).mean() - opt_mean) ** 2\n",
    "    std_loss = (opt.std(dim=-1).mean() - opt_std) ** 2\n",
    "    #reg_loss = norm_loss + mean_loss + std_loss\n",
    "    #reg_loss = torch.mean(torch.stack([(o - embed_weights).mean() for o in opt]))\n",
    "    reg_loss = torch.topk(torch.stack([torch.norm(o - embed_weights, dim=-1, p=3) for o in opt]), 1, largest=False).values.mean() * 0.02\n",
    "\n",
    "    reg_losses.append(reg_loss.item())\n",
    "    sim_loss = -1 * (torch.nn.functional.cosine_similarity(text_feats, img_feats)).mean()\n",
    "    \n",
    "    loss = sim_loss + reg_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # move opt back to orig stats - does not work\n",
    "    #with torch.no_grad():\n",
    "    #    opt = (opt - opt.mean(dim=-1, keepdim=True)) / opt.std(dim=-1, keepdim=True)\n",
    "    #    opt = (opt * opt_std) + opt_mean\n",
    "    \n",
    "    # calc non-augmented loss\n",
    "    with torch.no_grad():\n",
    "        non_aug_text_feats = model.encode_text(tokens, embedding=embedding)\n",
    "    non_aug_loss = -1 * (torch.nn.functional.cosine_similarity(non_aug_text_feats, norm_img_feats)).mean()\n",
    "    # log losses\n",
    "    aug_losses.append(sim_loss.item())\n",
    "    losses.append(non_aug_loss.item())\n",
    "    # decode text\n",
    "    current_text = decode_text(model, embedding)\n",
    "    # calc loss based on decoded text\n",
    "    decode_loss = -1 * (torch.nn.functional.cosine_similarity(model.encode_text(tokenize(current_text).to(device)), norm_img_feats)).item()\n",
    "    text_losses.append(decode_loss)\n",
    "    \n",
    "    if decode_loss < best_text_loss:\n",
    "        best_text = current_text\n",
    "        best_text_loss = decode_loss\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_description(current_text + \" - aug loss \" + str(round(loss.item(), 2)) + \" loss \" + str(round(non_aug_loss.item(), 2)) + \" decode loss \" + str(round(decode_loss, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.hist(opt[0].cpu().detach().flatten().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_loss = torch.topk(torch.stack([torch.norm(o - embed_weights, dim=-1, p=3) for o in opt]), 1, largest=False).values.mean()\n",
    "reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'ó charismatic herr sunsetgrayson' - for me\n",
    "#'\" landscapes voyage schelthur' - for autumn\n",
    "#'patient �📝: rito grows ' - for ouzi\n",
    "#'roomie stru<|startoftext|>saharan collie ' for ouzi\n",
    "#'photoshopped yikes .- hotdog watches ' - for hot-dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(model.encode_text(tokenize(best_text).to(device)), norm_img_feats).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(model.encode_text(tokenize(\"A picture of a landscape\").to(device)), norm_img_feats).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aug_losses, label=\"aug loss\")\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.plot(text_losses, label=\"text loss\")\n",
    "plt.plot(reg_losses, label=\"reg loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_text(model, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_losses = []\n",
    "\n",
    "#ds = torch.utils.data.TensorDataset(embed_weights)\n",
    "#dl = torch.utils.data.DataLoader(ds, batch_size=64)\n",
    "\n",
    "all_text_feats = []\n",
    "\n",
    "embed_weights = model.token_embedding.weight.data\n",
    "\n",
    "\n",
    "for token_emb in tqdm(embed_weights):    \n",
    "    embedding = embed.clone()\n",
    "    embedding[0, start_token:max_idx] += token_emb.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_feats = model.encode_text(tokens, embedding=embedding)\n",
    "        \n",
    "    all_text_feats.append(text_feats)\n",
    "    \n",
    "    #loss = -1 * (torch.nn.functional.cosine_similarity(text_feats, norm_img_feats)).detach()\n",
    "    #token_losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = torch.stack(all_text_feats).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(feats.cpu(), \"feats.pt\")\n",
    "#feats = torch.load(\"feats.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.nn.functional.cosine_similarity(feats, norm_img_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_text(model, feats[torch.argmax(losses).unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200\n",
    "best_tokens = torch.topk(losses, k).indices.cpu().numpy()\n",
    "decoded_text = tokenizer.decode(best_tokens)\n",
    "print(torch.topk(losses, k).values.cpu().numpy())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_text(tokens, embedding=embed_weights[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Let me tell you something about this girl named Emilia Wiehe.\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(past_key_values=torch.zeros(5, 2048), do_sample=True, temperature=0.9, max_length=100, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer.encode(\"My feet\")\n",
    "context = torch.tensor([generated])\n",
    "past = None\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    output = model(context, past_key_values=past, use_cache=True)\n",
    "    logits = output[\"logits\"]\n",
    "    past = output[\"past_key_values\"]\n",
    "    token = torch.argmax(logits[..., -1, :])\n",
    "\n",
    "    generated += [token.tolist()]\n",
    "    context = token.unsqueeze(0)\n",
    "\n",
    "sequence = tokenizer.decode(generated)\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in past:\n",
    "    for ten in vec:\n",
    "        ten.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(past_key_values=past, do_sample=True, temperature=0.9, max_length=100, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(opt_steps):\n",
    "    text_feats = model.encode_text(opt_tokens.cuda())\n",
    "    \n",
    "    loss = -1 * torch.nn.functional.cosine_similarity(text_feats, img_feats)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    print(opt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with Fil",
   "language": "python",
   "name": "filprofile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
