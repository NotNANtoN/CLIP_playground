{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(\"../StyleCLIP_modular\")\n",
    "from style_clip import Imagine, create_text_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "parser.add_argument(\"--gradient_accumulate_every\", default=1, type=int)\n",
    "parser.add_argument(\"--save_every\", default=1, type=int)\n",
    "parser.add_argument(\"--epochs\", default=1, type=int)\n",
    "parser.add_argument(\"--story_start_words\", default=5, type=int)\n",
    "parser.add_argument(\"--story_words_per_epoch\", default=5, type=int)\n",
    "parser.add_argument(\"--style\", default=\"../stylegan2-ada-pytorch/VisionaryArt.pkl\", type=str, choices=[\"faces (ffhq config-f)\", \"../stylegan2-ada-pytorch/VisionaryArt.pkl\"])\n",
    "parser.add_argument(\"--lr_schedule\", default=0, type=int)\n",
    "parser.add_argument(\"--start_image_steps\", default=1000, type=int)\n",
    "parser.add_argument(\"--iterations\", default=100, type=int)\n",
    "args = vars(parser.parse_args({}))\n",
    "\n",
    "args[\"opt_all_layers\"] = 1\n",
    "args[\"lr_schedule\"] = 1\n",
    "args[\"noise_opt\"] = 0\n",
    "args[\"reg_noise\"] = 0\n",
    "args[\"seed\"] = 1\n",
    "\n",
    "args[\"model_type\"] = \"vqgan\"\n",
    "args[\"iterations\"] = 200\n",
    "args[\"save_every\"] = 1\n",
    "args[\"start_img_loss_weight\"] = 0.0\n",
    "\n",
    "args[\"lr\"] = 0.1\n",
    "\n",
    "#run(img=\"base_images/aicpa_logo_black.jpg\", start_image_path=\"base_images/stance.jpg\", args=args)\n",
    "#run(img=\"base_images/aicpa_logo_black.jpg\", start_image_path=\"base_images/earth.jpg\", args=args)\n",
    "#run(img=\"base_images/earth.jpg\", start_image_path=\"base_images/aicpa_logo_black.jpg\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"start_image_steps\"] = 500\n",
    "args[\"sideX\"] = 320\n",
    "args[\"sideY\"] = 320\n",
    "\n",
    "path_dict = {\"logo_black\": \"base_images/aicpa_logo_black.jpg\",\n",
    "        \"logo_purple\": \"base_images/aicpa_logo_purple.jpg\",\n",
    "        \"earth\": \"base_images/earth.jpg\",\n",
    "        \"stance\": \"base_images/stance.jpg\",\n",
    "        }\n",
    "latent_dict = {}\n",
    "\n",
    "for key in path_dict:\n",
    "    print(key)\n",
    "    args[\"start_image_path\"] = path_dict[key]\n",
    "    imagine = Imagine(\n",
    "                save_progress=False,\n",
    "                open_folder=False,\n",
    "                save_video=False,\n",
    "                **args\n",
    "               )\n",
    "    img = imagine.prime_image()\n",
    "    latents = imagine.model.model.latents.detach().cpu()\n",
    "    latent_dict[key] = latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earth, logo_black, logo_purple, stance\n",
    "\n",
    "start = \"earth\"\n",
    "end = \"stance\"\n",
    "steps = 100\n",
    "\n",
    "start_latent, end_latent = latent_dict[start], latent_dict[end]\n",
    "\n",
    "# interpolate\n",
    "# Obtain evenly-spaced ratios between 0 and 1\n",
    "linspace = torch.linspace(0, 1, steps)\n",
    "\n",
    "# Generate arrays for interpolation using ratios\n",
    "latent_transition = [(1 - l) * start_latent + l * end_latent for l in linspace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images from latents\n",
    "images = []\n",
    "for latent in tqdm(latent_transition):\n",
    "    model = imagine.model.model\n",
    "    model.latents = latent.to(\"cuda\")\n",
    "    image = model(return_loss=False)\n",
    "    image = image.detach().cpu().squeeze(0).permute(1, 2, 0).clamp(0, 1) * 255\n",
    "    image = image.type(torch.uint8).clamp(0, 255)\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"transitions\"\n",
    "os.makedirs(folder, exist_ok=1)\n",
    "path = os.path.join(folder, f\"{start}_to_{end}_{steps}.mp4\")\n",
    "imageio.mimwrite(path, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-struggle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
